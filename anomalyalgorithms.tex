\section{Anomaly Detection Methods}
\label{sec:methods}

For this investigation we developed various anomaly detection methods, ranging from simple statistical algorithms to tests of a more advanced nature. These more advanced tests involved reconstructions of original data based on knowledge of a training set of data.

In addition to this, peak-fitting in Fourier space and a visual inspection of harmonics was also implemented.

As part of the data reduction procedure, a moving average was applied to data in both the time domain and the frequency domain. A moving average essentially applies a low-pass filter to the data.

For the time domain, this was mainly applied in order to ensure more accurate reconstructions when using techniques such as K-mean Clustering Reconstruction (see \S~\ref{subsec:kmeans}) and LSTM Recurrent Neural Networks (see \S~\ref{subsec:LSTM}).

The moving average was applied to the frequency domain, after the Fourier transform has already been applied to the unaltered time series data. This ensures that all of the higher frequencies were not lost.

The method of taking a moving average is implemented using the convolution of the data with a kernel, $k$. If a moving average with width $w$ is required, the kernel is defined as,

\begin{equation}
    k = \left[ \dfrac{1}{w}~\dfrac{1}{w}~\dfrac{1}{w}...\dfrac{1}{w}  \right]~,
    \label{eq:conv_kernel}
\end{equation}

where the array is of length $w$. This is then convolved with the data set, which has the effect of `smoothing' the data.

\begin{figure}[t]
    \includegraphics[width=1.0\textwidth]{fig/moving_average.pdf}
    \caption[Time Domain]{Moving average applied to data in the time domain. The original data (shown in blue) represents the amplitude of the signal as a function of time. The orange curve shows the moving average of this data.}
    \label{fig:moving_av}
\end{figure}

\begin{figure}[t]
    \includegraphics[width=1.0\textwidth]{fig/freq_moving_average.pdf}
    \caption[Moving Average in Fourier Domain]{Moving average applied to data in the Fourier domain, shown in both linear and logarithmic axes. The blue lines show the original amplitudes as a function of frequency. The orange lines show the moving averages.}
    \label{fig:freq_moving_av}
\end{figure}

Figures.~\ref{fig:moving_av} and ~\ref{fig:freq_moving_av} show examples of applying a moving average to data in the time domain and frequency domain, respectively. 


%----Moving average for dealing with issues with the resolution for the raw data in the y axis.
%----Change figure in time domain to show resolution problem and how this was sorted using moving average

\subsection{Statistical Approach}

The most simple methods for detecting changes from `normal' motor behaviour can either use simple value thresholds, or static mean and standard deviation calculates to determine when data experiences a significant deviation from usual patterns. 

Such basic approaches are not easily adapted for use in time series data. This is because:

\begin{enumerate}
    \item These methods are not easily applied to changing data values.
    \item They cannot scale easily to large time series.
    \item An unacceptable quantity of false anomalies can be detected.
    \item They rely on the assumption of a normal distribution of data points, which is not always the case for time series data.
\end{enumerate}

Nevertheless, these approaches can provide immediate indications of the most obvious anomalies. 

In order to ensure that the data to which the statistical tests were applied followed a Gaussian distribution, a moving average was first taken.

\begin{figure}[t]
    \includegraphics[width=1.0\textwidth]{fig/moving_av_hist.pdf}
    \caption[Moving Average Histogram]{Histograms for the raw data (top) and when a moving average with a window size of $w=20$ is applied (bottom).}
    \label{fig:move_av_hist}
\end{figure}

Figure.~\ref{fig:move_av_hist} shows how the distribution for the moving-averaged data has a Gaussian distribution, whereas the raw data does not.

It was therefore necessary to apply a moving average to the raw data before any statistical tests were performed.

\subsubsection{Histogram Test}

\begin{figure}[t]
    \includegraphics[width=1.0\textwidth]{fig/histogram.pdf}
    \caption[Histogram]{Histogram method}
    \label{fig:histogram}
\end{figure}

The histogram test first assumes that the recorded data is normally distributed, so that the probability $p$ of a value $x$ is,

\begin{equation}
    p(x) = \dfrac{1}{\sigma \sqrt{2\pi}}\exp(\dfrac{-{(x-\mu)}^2}{2\sigma^2})~,
\end{equation}

where $\mu$ and $\sigma$ are the mean and standard deviation of the sample respectively.

The method begins by separating the `training' time series sample into equal bins. This training sample serves as a set of example data that represents the expected behaviour. The median value of each of these bins is then calculated.

A reconstruction of a time series is then carried out. For each value in the time series, the difference between the defined medians, and the value is calculated. The smallest difference is then chosen to represent the reconstruction of the time series.

Figure.~\ref{fig:histogram} shows an example of the histogram method for an arbitrary time series. The reconstruction error represents the difference between the original time series and the reconstructed time series. The training data was taken from a motor operating at the same voltage as the test data, under normal baseline conditions.

A typical anomaly alert with this test would typically be set to trigger if the reconstruction error exceeds the threshold of 99th percentile of our reconstruction error.

\subsubsection{Grubbs' Test}
One of the most commonly used statistical threshold tests for outliers is the two-sided Grubbs' Test \cite{CIS-320511}.

For the two-sided Grubbs' test, this involves defining a Grubbs' test statistic as,

\begin{equation}
    G = \dfrac{\underset{i=1,2,...,N}{\max}|\mathcal{X}_i-\bar{\mathcal{X}}|}{\sigma}~,
    \label{eq:grubbs}
\end{equation}

where $\mathcal{X}$ denotes a set of data of length $N$, and $\bar{\mathcal{X}}$ and $\sigma$ are the mean and standard deviation of this data set, respectively. $G$ is equivalent to the largest deviation from the mean in units of standard deviations.  

The hypothesis of no outliers being present is rejected if,

\begin{equation}
    G > \dfrac{N-1}{\sqrt{N}} \sqrt{\dfrac{(t_{\alpha/(2N), N-2})^2}{N-2+(t_{\alpha/(2N), N-2})^2}} ~,
    \label{grubbs_condition}
\end{equation}

where $t_{\alpha/(2N), N-2}$ denotes the $t$-distribution with a significant level of $\alpha/(2N)$ and $(N-2)$ degrees of freedom. 

The Grubbs' test is useful in identifying if an outlier exists in the data set, however it is very limited. It only identifies if the data is anomalous, and does not indicate how many anomalies there are or indicate where in time the anomalies are.

Nevertheless, it can be used as a very quick indication of whether results are anomalous in amplitude over time. If it indicates outliers are present, more rigorous statistical tests can be used to determined where and when an outlier occurred. 

\subsection{Median Absolute Deviation}

The Median Absolute Deviation (MAD) is defined as the median of the residuals of a univariate data set, $\mathcal{X}$, from the median value of the data set \cite{rousseeuw1993alternatives},

\begin{equation}
    MAD = median~(|\mathcal{X}_i - median(\mathcal{X})|)~.
    \label{eq:MAD}
\end{equation}

In this work, a data point was defined as anomalous if it deviated from the median by more than $20 \times MAD$.

\subsubsection{Standard Deviation Test}
Assuming that a data set is normally distributed, a simple threshold test on the residuals from the mean can be used to detect outliers. 
Although a deviation of more than $3\sigma$ can in some cases be considered to be anomalous, there is still a $0.3\%$ chance of a data point following the normal distribution to fall more than $3\sigma$ from the mean. In some cases, the data sets analysed in this report can exceed $100,000$ entries. Using a $3\sigma$ threshold test in this case would lead to $300$ false positives. Alternatively, with a sampling rate of $3000$ Hz, this would equate to $9$ incorrect outlier alerts every second. 

Instead, $5\sigma$ was used as the threshold for the residuals. The likelihood of a normal data point deviating from the mean by more than $5\sigma$ is $3 \times 10^{-7}$, giving a much lower false positive detection rate of one every $\sim18$ minutes. 

Therefore for the purpose of this report, a data point deviating by more than $5\sigma$ from the mean is considered to be an anomaly. 

This method can be used to find multiple anomalies within a data set, with their respective positions in time.

\subsubsection{Deviation From Moving Average}
This is a similar method to the standard deviation test, but instead of calculating the mean and standard deviation of the entire sample, a moving mean and standard deviation are taken. A window size of 50 for the moving average was used.
If a data point deviates from the moving average by 4 moving standard deviations or more, then it is considered to be an outlier. This method is better for detecting just the anomalies at shorter time scales, whereas the Standard Deviation Test detects both short scale and more gradual amplitude changes. 

Unlike the previous two tests, this is applied to the raw data, as the test itself involves smoothing the data and comparing this to the raw data.


\subsection{Fourier Analysis}

\begin{figure}[t]
    \includegraphics[width=1.0\textwidth]{fig/freq_large_4Vnowater.pdf}
    \caption[Fourier Spectrum of 12 V motor in $xyz$ axes]{Amplitudes as a function of frequency for the 12 V motor in all 3 axes. }
    \label{fig:frequencies}
\end{figure}

Fourier transforms of time series data are used to provide a mapping between the time domain and the frequency domain, finding the amplitude of the constituent frequencies \cite{hsu_1984}. Another term for this frequency domain representation is the spectrum of the signal. 

This transformation was implemented using a Fast Fourier Transform (henceforth FFT), in order to perform a discrete fourier transform (DFT). If time series data consists of a set of data of length $N$, $\{ x_0, x_1,...,x_{N-1} \}$, the DFT is defined as \cite{hsu_1984},

\begin{equation}
    y_k = \sum_{m=0}^{N-1} x_m \cdot e^{-2\pi ikm/N} \quad (k = 0,1,...,N-1)~,
    \label{eq:DFT}
\end{equation}

where $y_k$ is the amplitude for a given $k$. $k$ is related to frequency, and the frequency $f$ can be obtained using the equation,

\begin{equation}
    f = \dfrac{k}{N} \times S~,
    \label{eq:freq_from_k}
\end{equation}

where $S$ is the sampling rate with which the data was collected ($S=3000$ Hz for this work). Therefore Equation.~\eqref{eq:DFT} returns values of $y_k$, for $k = 0,1,...,N-1$, making $N$ values in total, that correspond to amplitudes for equally spaced frequencies between $0$ and $3000$ Hz. However, due to the Nyquist-Shannon Sampling Theorem (see \S\ref{subsec:anomaly_detection}), only the first half of these frequencies can be properly resolved, and so the second half of the $y_k$ values are discarded.

When representing the spectrum, the power spectral density (PSD) was used, which is found by multiplying the complex amplitude obtained from the $FFT$ by its complex conjugate, giving $|FFT|^2$. This is useful as it is proportional to the power output per unit frequency, and in addition makes peaks more visually prominent in the spectrum. The $|FFT|^2$ was then normalised by dividing by $N$, the length of the data, to ensure the amplitudes of peaks in Fourier space do not depend on the length of data used to generate them (so that a spectrum taken from 2 seconds of data will have the same amplitudes as one taken from 2 minutes of data).

Figure.~\ref{fig:frequencies} shows the power spectral density returned using this method. A logarithmic axis is used to make the less prominent peaks visible.

\subsubsection{Gaussian fitting in frequency space}

In addition to visual inspection of the spectra, a more quantitative approach was attempted. This was achieved by fitting a Gaussian curve to the most prominent peak in the spectrum. This was done by varying the parameters of the Gaussian and using a least-squares regression algorithm to minimise the residuals between the data and the Gaussian curve. A Lorentzian curve was also attempted, but the Gaussian generally gave a smaller sum of residuals and so is the method used. 

A Gaussian peak is defined by,

\begin{equation}
   \mathcal{G}(f) = A e^{-(f-\mu)^2/2\sigma^2}~,
    \label{eq:gaussian}
\end{equation}

where $A$ is the amplitude of the Gaussian, equal to the maximum amplitude of the peak; $\sigma$ describes the width of the peak and is related to the full-width at half-maximum (FWHM) by FHWM = $2\sigma \sqrt{2ln2}$, and $\mu$ is the frequency of the centre of the peak. The total area under the peak is given by $\sqrt{2\pi}\sigma A$.

Time series data from a "healthy" motor (one in which no possible faults have been purposefully imposed) were subdivided into segments and spectra obtained in all three motor axes. These were used to generate a set of Gaussians for the most prominent peak in each, each described by the three parameters $A$, $\sigma$ and $\mu$, defined above.

Initially, baseline data was split into three second segments and a Fourier spectrum was obtained for each segment. A Gaussian curve was fitted to the most prominent peak in each spectrum. This was used to create a database of values for $A$, $\sigma$ and $\mu$ from each segment. From this, a mean and standard error were obtained for $A$, $\sigma$ and $\mu$. 

This process was then repeated for data that was recorded when trying to induce failures, in order to again determine values for $A$, $\sigma$ and $\mu$ with their respective errors. If any of $A$, $\sigma$ or $\mu$ do not agree within their errors with the values obtained from the "healthy" baseline data, then there is an indication that the frequency response of the motor is changing. This technique can be very useful in autonomously tracking the frequency response of a motor. As many failure modes can manifest themselves as gradual changes in the frequency of the motor, this technique can be useful for detecting signs of failure that the statistical tests applied to amplitudes in the time domain fail to identify.

%----Include figure of Gaussian fit
 
\subsubsection{Visual Inspection}
Abnormal motor behaviour can manifest itself in a variety of ways in the Fourier spectrum. Changes in the amplitude, position and width of the fundamental frequency have already been discussed, but movement of peaks with lower amplitudes, and in particular harmonics, also present very important information. Figure~\ref{fig:frequencies} shows there are clear examples of these harmonics in the y and z axes.

Due to the lower dynamic range and greater variability of these secondary peaks, it was difficult to reliably fit Gaussians to the lower amplitude peaks, and so a more visual inspection of the spectra is employed instead. A logarithmic $|FFT|^2$ axis is used more easily notice the lower amplitude peaks.

\subsection{Least Squares Anomaly Detection}
Included amongst the anomaly detection methods is a squared-loss objective function which is quick to detect anomalous data given a set of training and testing data sets. Due to the rudimentary nature of the function and the very simple analytical solution, it has a speed advantage over other methods, as well as presenting an anomaly score across the train and test data to visualise the magnitude of the anomalous data points. It can also be used to evaluate the difference between sub-sequences and therefore isn't invalidated when the data set contains structural dependence.

Initially a moving average was applied to the data to low pass it before median filtering. The class-conditional probability of the data point,  $p(y|\mathbf{x})$, was then determined for $p(y=i|\mathbf{x}, \mathbf{\theta_i})$ using equations $q(y=i|\mathbf{x}$, $\mathbf{\theta_i})$ for each $i \in \chi$, where $\chi = \{1, ...., c\}$ is the set of all possible classes.
It is achieved by using the approximations,

\begin{equation}
q(y=i|\mathbf{x},\mathbf{\theta_i}) = \mathbf{\theta_i}^T\mathbf{\phi}(\mathbf{x})~,
\end{equation}

where $\mathbf{\theta_i} = (\theta_{i,1},...,\theta_{i,N})^T \in \mathbb{R}^N$ and $\mathbf{\phi}(\mathbf{x})$ is a vector comprising basis function kernels.

The model was fit using,

\begin{equation}
    J_i(\mathbf{\theta_i})=\dfrac{1}{2}\int(q(y=i|\mathbf{x},\mathbf{\theta_i}) - p(y=i|\mathbf{x}))^2p(\mathbf{x})d\mathbf{x}~.
\end{equation}

Using the classes of data defined by the class-conditional probabilities of the data points, we can determine the class-conditional probability of anomalous data by making similar assumptions to those made in a one-class support vector machine regarding the distribution in the test data relative to the classes used within the training data. Assumptions such as the outliers are restricted to exist in low-density data-space, and using the training data, a kernel model can describe high-density regions.

These assumptions can be coded using,
\begin{equation}
    q(y=\ast|\mathbf{x},\mathbf{\theta_{\ast}}) = 1 - \mathbf{\theta_{\ast}}^T\mathbf{\phi}(\mathbf{x})~,
        \label{lsanom1}
\end{equation}

to estimate the conditional probability of an outlier $p(y=\ast|\mathbf{x},\mathbf{\theta_i})$.

We can then locate the outliers by determining $\mathbf{\theta_{\ast}}$, achieved by equating Equation.~\eqref{lsanom1} close to one in a low-density region of data and zero in a high-density region of data and minimising,

\begin{equation}
\mathbf{J_{\ast}}(\mathbf{\theta_{\ast}}) = \dfrac{1}{2}\int(1 - \mathbf{\theta_{\ast}}^T\mathbf{\phi}(\mathbf{x}))^2p(\mathbf{x})d\mathbf{x} + \dfrac{\rho}{2}\|\mathbf{\theta_{\ast}}\|^2~,
\end{equation}

across the data set, where an increase in $\rho$ provides an increase in sensitivity to outliers. Inside outlying areas where $\phi(\mathbf{x})$ approaches $\phi(\mathbf{x}=0)$, Equation.~\eqref{lsanom1} approaches 1 regardless of $\mathbf{\theta_{\ast}}$. Using an empirical approximation for a finite data set containing training data finally produces,

\begin{equation}
q(y=\ast|\mathbf{x}, \mathbf{\widehat{\theta_{1}}},..., \mathbf{\widehat{\theta_{c}}}) = 1 - \sum_{j \in \chi}^{} q(y=j|\mathbf{x}, \widehat{\theta_{j}})~,
\end{equation}

An approximation to the conditional probability of a data point belonging to any of the states provided by the training data $p(y_t \in S|\mathbf{x}_t)$ is the sum of the estimator terms for all $i \in S$. An estimation of the conditional probability of a data point being inside an unknown state can be considered from the complement of this sum, $p(y_t \notin \chi|\mathbf{x}_t)$.

This method therefore identifies anomalous results by assigning probabilities of the data point being found within a known class within the training data and using the density of such data to predict the probability of a result being anomalous. 

\begin{figure}[t]
    \includegraphics[width=1.0\textwidth]{lsanomaly.pdf}
    \caption[Least Squares Anomaly Score]{Plot detailing positions of anomalous data as detected by the Least Squares Anomaly Detection method.}
    \label{fig:lsanomaly}
\end{figure}

\subsection{K-Means Clustering}
\label{subsec:kmeans}

Cluster analysis involves combining data points into groups, or clusters, in such a way that the points in each cluster are more similar to each other than other points outside of the cluster. Any data points that do not belong to a cluster are defined as outliers. 

Figure.~\ref{fig:anomalies} in \S \ref{subsec:anomaly_detection} shows an example of clusters in 2 dimensions. 

K-means clustering is one of the most highly used cluster analysis methods, due to it being much less computationally intensive in comparison to other approaches \cite{Kanungo:2002:EKC:628329.628801}.

The process involves partitioning a set of n-dimensional data points, of length $N$ ($\{\underline{x}_j\}$ for $j=1,2,...,N$) into a group of $k$ clusters, where $k$ has to be specified by the user. K-means aims to find the centroids $\underline{\mu}_i$ (with $i=1...k$), of the clusters that minimise the Euclidean distance from all data points to their respective clusters, $d(\underline{x}, \underline{\mu}_i) = ||\underline{x}-\underline{\mu}_i||^2$. More formally, K-means clustering finds the centroids of $k$ clusters in order to minimise the expression \cite{596afe3f2b5a4ff3b8f4f9793ad2f4ee},

\begin{equation}
    arg\,min_k \sum_{i=1}^{k} \sum_{\underline{x} \epsilon c_i} ||\underline{x}-\underline{\mu}_i||^2 ~,
    \label{eq:K-means}
\end{equation}

where $c_i$ are the points closest to the cluster $i$.

The general process is described as follows:
\begin{enumerate}
    \item Initialise the centre of the $k$ clusters randomly in Euclidean n-space.
    \item Attribute each data point to the closest cluster centre for that data point. 
    \item Assign a new position for the cluster centre, now given by the barycentre of the data points belonging to the cluster.
    \item Repeat steps 2 and 3 until convergence to a solution is achieved.
\end{enumerate}

In order to apply K-means clustering to time series data, the n-dimensional space in which the clusters are defined first had to be defined. 

The entire signal waveform was split into segments in time, with each segment consisting of 24 data points. These segments will then be defined by a set of features, which will form the n-dimensional space. For example, the features could be the mean value and standard deviation of each segment, in which case a 2-dimensional space would be considered. Instead, in order to ensure no information is lost, each element in the 24-element segment is taken to be a separate dimension. Therefore a segment is completely specified by a single point in 24-dimensional space \cite{lin_vlachos_keogh_gunopulos_2004}.

All of the segments that make up the total waveform can then be plotted in this 24-dimensional space. Segments with similar features will cluster together. If $k$ clusters are defined, then the centroids of each cluster can be found to give $k$ coordinates in the 24-dimensional space, one for each centroid. Figure.~\ref{fig:kmeans_training} shows ten random segments, taken from the original waveform, also known as the "training data".

The centroids, being points in the defined 24-dimensional space, are themselves waveform segments. These waveform segments will be referred to as synthetic segments, as they were generated from the input data but aren't themselves a part of the original data. 

\begin{figure}[t]
    \includegraphics[width=1.0\textwidth]{fig/kmeans_training.pdf}
    \caption[K-means training segments]{A random selection of training segments used for the K-means method, with windowing applied.}
    \label{fig:kmeans_training}
\end{figure}

\begin{figure}[t]
    \includegraphics[width=1.0\textwidth]{fig/kmeans_synthetic.pdf}
    \caption[K-means synthetic segments]{A random selection of synthetic segments, generated from the centroids of clusters returned from the K-means method.}
    \label{fig:kmeans_synthetic}
\end{figure}

This creates a ``library'' of $k$ synthetic waveforms, which can be stitched together in order to recreate the original total signal. Figure.~\ref{fig:kmeans_synthetic} shows a random selection of ten of these synthetically generated waveforms.

Figure.~\ref{fig:kmeans_training} shows that all of the segments begin and end with an amplitude of $0$. This is as a result of a process known as "windowing" being applied. Windowing is the process of multiplying the training segment by a sine squared function of the form $\sin^2(\pi/N_E)$, where $N_E$ is the number of elements per segment. It is necessary to ensure that all of the training segments begin and end with an amplitude of $0$, so that the learned synthetic segments also have zero amplitude at both ends. This ensures no discontinuities when stitching together the synthetic segments to recreate the entire signal waveform. 

The process creating a reconstruction of a waveform once the synthetic segments are obtained is as follows:

\begin{enumerate}
    \item Split the training waveform into segments of length $N_E$ (same length as before). Have the start each segment at 2 elements later than the previous one, to ensure a large overlap between segments.
    \item From these segments, multiply each one by the sine windowing function and find the centroids in $N_E$-dimensional space ($N_E$ taken to be 24 as before) to create a library of $k$ synthetic segments. This functionality was provided by the python package sklearn.
    \item Split the testing data (the data to reconstruct) into segments containing $N_E$ elements each. This time ensure that each segment overlaps by $N_E/2$ so that, for example, the first half of the second segment is taken from the same part of the waveform as the second half of the first segment.
    \item For each testing segment, find the synthetic segment that is the minimum Euclidean distance away in $N_E$-dimensional space, thus describing the closest fit. 
    \item Add the synthetic segments together to recreate the entire testing waveform. As the testing segments are separated by $N_E$, there is an overlap of half a segment for each segment. For the windowing function, $\sin^2(\pi/N_E)$, this corresponds to a change of phase of $1/4$ of a wavelength, or simply $\cos^2(\pi/N_E)$. Therefore when adding the two segments together, the windowing functions cancel to $\sin^2(\pi/N_E) + \cos^2(\pi/N_E) = 1$. This ensures there is no change in amplitude for any element that is a part of two overlapping segments.
\end{enumerate}


Note that this reconstruction can be of a waveform different to the waveform used as training data. The signal waveform attempted to be reconstructed will henceforth be referred to as the "testing data". This allows for a method known as supervised K-means reconstruction \cite{596afe3f2b5a4ff3b8f4f9793ad2f4ee}, in which synthetic segments are generated from baseline data, and then these segments can be used to create attempted reconstructions of data which was taken when trying induce failures. When the testing data differs from the training data however, a very important step is to multiply the testing data by a factor to ensure it has the same average amplitude as the training data - otherwise very large reconstruction errors will result.


\begin{figure}[t]
    \includegraphics[width=1.0\textwidth]{fig/kmeans2.pdf}
    \caption[K mean clustering plot]{An example of the K-means reconstruction method. The original signal (top) is reconstructed (middle) from a library of synthetic segments, generated from a set of training data from a "healthy" motor. The reconstruction error (bottom) is the difference between the reconstructed and original signal.}
    \label{fig:kmeanerror}
\end{figure}

Figure.~\ref{fig:kmeanerror} shows an example of a reconstruction of testing data from synthetic segments created from a baseline signal of training data. The reconstruction error is the absolute difference between the reconstructed testing data and the actual testing data. Any spike in this reconstruction error, or a high average value over the entire length of the waveform, indicates that the motor is behaving differently from its optimum operation assumed from the baseline run.

\subsection{LSTM Recurrent Neural Networks}
\label{subsec:LSTM}

\subsubsection{Description of LSTM architecture}

%Machine learning is the science of programming computers in a manner that means they carry out tasks while not being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome.

Artificial neural networks (ANNs) are a biologically-inspired paradigm that provide a powerful set of techniques that provide solutions to many modern-day problems. In an age of rapidly increasing computational power from graphic processing units, ANNs are rapidly increasing in popularity.

ANNs are best described as a computing system made of many simplistic but highly interconnected layers, which process information by their dynamic state response to an external input. They are `connectionist machines' that simulate the densely interconnected cells that are found in a brain.

A form of ANN known as a recurrent neural network (RNN) consists of a structure of hundreds of thousands of units arranged in layers, which connect to other layers on either side. Some of these units are known as input units - with their role being to receive an information input that the network will attempt to learn about and process. Other types of unit on the other side of the network attempt to form a response to a learned input; these are known as output cells. In between the input and output units are hidden units, which make up the vast majority of the network. Similar to the manner in which neurons in a real brain operate, the links between separate units are assigned weights, in which a positive weight excites the connected unit, and a negative weight suppresses and inhibits the other unit.

RNNs operate by taking an input of information via the input units, triggering layers of hidden units before ending up at the output units. Each unit receives an input from the units that are located earlier in the network. These inputs are multiplied by the weights associated with the connections. The training process for a RNN typically involves the use of a feedback mechanism that propagates the network error backwards through the system.

%HOW THEY LEARN HOW THEY BACKPROPAGATE. WHAT MAKES A RECURRENT

Typical RNNs employ backpropagation training alongside stochastic gradient descent optimisation methods. The combination of these is known as a `backpropagation through time' algorithm. 

What this method involves is an input being passed forward through the neural network, layer by layer, until it reaches the output layer. The network's output is compared to the targeted output with the use of a loss function. An error value is then assigned to each of the `neurons' in the output layer. These error values are then passed backwards through the network, until each layer has an error associated with it that is proportional, and therefore representative of its contribution, to the original output \cite{Graves2012}. The effect of this process is to reduce the difference between the actual and intended output, by modifying the weights of the connections throughout the network. This means the network will be able to `figure something out' from an input, based on how the network has been trained beforehand.

However one problem with RNNs is that the error signal suffers from an exponential decay as it propagates through the network. This means that the front layers train very slowly, with the overall effect being that recurrent neural networks fail to learn in the presence of long lags in time between an input and target events. This problem was formalised by researchers and is known as the `vanishing gradient problem' \cite{hochreiter1991untersuchungen, hochreiter2001gradient}. By the mid-1990s the vanishing gradient problem had emerged as a major obstacle in the performance of recurrent neural networks.

LSTM (long short-term memory) is a type of RNN that was first proposed by \cite{Hochreiter:1997:LSM:1246443.1246450} as a way to overcome the vanishing gradient problem. An LSTM network is extremely well suited for learning tasks such as speech recognition \cite{Graves2005602}, handwriting recognition \cite{graves2008unconstrained} and music composition \cite{eck2002learning}. The properties of LSTM networks also make them extremely valuable for the prediction of time series \cite{gers2001applying}.

An LSTM consists of many recurrently connected subnets known as memory blocks. A memory block consists of three gates: the input, forget and output, as well as the cell state \cite{gers2000learning}. The core idea behind LSTMs is the cell state - which acts like a a conveyor belt. It lets information flow across the memory block unchanged, with the exception of a few minor linear interactions. These interactions are the gates - responsible for the removal or addition of information to the cell state (see Figure.~\ref{fig:lstmcell}).

%the optionally let memory through. each is is a sigmoid neural network and a multiplication operation the sigmoid function outputs a values between 0 and 1 which describes how much of each component should be let through we represent each of the following gates through the following equation.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{LSTMcell.pdf}
    \caption[LSTM cell schematic]{Schematic of a long short-term memory block, used as a `neuron' in the hidden layers of a recurrent neural network.}
    \label{fig:lstmcell}
\end{figure}

A more mathematical description of the LSTM on how a layer of memory cells is updated begins with an input $x_t$ at timestep $t$. We define weight matrices as $W_i$, $W_f$, $W_c$, $W_o$, $U_i$, $U_f$, $U_c$, $U_o$ and bias vectors $b_i$, $b_f$, $b_c$, $b_o$.

At a time $t$, we define the input gate value $i_t$ and candidate value for the state of the memory cells $\widetilde{C}_t$ with,

\begin{equation}
    i_t = \sigma(W_i x_t + U_i h_{t-1} + b_i)~,
\end{equation}
\begin{equation}
    \widetilde{C}_t = \tanh(W_c x_t + U_c h_{t-1} + b_c)~.
\end{equation}

Next we calculate $f_t$, the value for the activation of the memory cells' forget gates,

\begin{equation}
    f_t = \sigma (W_f x_t + U_f h_{t-1} +b_f)~,
\end{equation}

where $\sigma$ is the activation function.

With these computed values we can then calculate $C_T$, the new state for the memory cells.

\begin{equation}
    C_t = i_t * \widetilde{C}_t + f_t * C_{t-1}~,
\end{equation}

And finally, one can now compute the value of the output gates and therefore outputs of the memory cells,

\begin{equation}
   o_t = \sigma(W_o x_t + U_o h_{t-1} + V_0 C_t + B_0)~,
   \label{output1}
\end{equation}
\begin{equation}
    h_t = o_t * \tanh(C_t)~.
\end{equation}

In the interest of more efficient computation, the LSTM model used in this report is a slight variant from what is most commonly described \cite{Graves2012}. The activation of a particular memory cell's output gate doesn't depend on the memory cell's state. This has the result of taking $V_0 = 0$ in Equation.~\eqref{output1}.

\subsubsection{LSTM implementation for anomaly detection}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{neuralnetwork.pdf}
    \caption[Neural Network]{Normalised data reconstructed using an LSTM model trained on baseline data. The mean squared error is the square of the difference between the original and reconstructed data.}
    \label{fig:neuralnet}
\end{figure}

The LSTM results shown in Figure.~\ref{fig:neuralnet} were achieved with the use of a high-level neural network library named \texttt{Keras}. This operated using a \texttt{TensorFlow} backend, a recently open-sourced framework released by Google in order to aid in the implementation of large-scale learning models \cite{Abadi:2016:TSL:3026877.3026899}.

The description of how the LSTM was programmed is as follows. We first build a sequential model, meaning that layers are stacked in the object.  We define a network with a 1D input, with three hidden layers of sizes 64, 256 and 100, and end with a 1D output layer. We take testing and training data, normalised so that the mean of the dataset is 0 and the standard deviation is 1.

With the model initialised, we define our first LSTM layer that behaves as a standard recurrent layer. We add a 20\% dropout to this layer. 

Dropout is a regularisation method, that when used correctly, greatly reduces overfitting in LSTMs \cite{2014arXiv1409.2329Z}. Dropout involves randomly removing units from the network during the training process \cite{2013arXiv1311.0701B}.

An additional two layers are then added, again with a dropout of 20\%. For the last layer we choose a dense feedforward layer with a linear activation. We compile the model using a mean square error using the \texttt{rmsprop} gradient descent optimisation algorithm \cite{tieleman2012lecture}.

The network is trained for one epoch (there is one complete pass through the dataset).

LSTMs are proving the ultimate tool in modern-day time series forecasting and subsequent anomaly detection. However, as with any method, there are disadvantages.

Firstly, they can be described as the ultimate `black boxes'. This means that apart from setting up the general structure of a network and seeding its initial conditions, the operator of an LSTM has no other role than to supply and input, train the network, and await the output. The end product of this activity is a trained neural network that provides no relationship described with the use of an equation or set of coefficients beyond its internal mathematics. The network therefore \emph{is the final representation} of the relationship.

Furthermore, LSTMs tend to be slower to train than other types of neural networks. This can also be computationally intensive and time consuming.

Overfitting is a major problem with recurrent neural networks. When we train a complex model is can be very easy to fit the network reconstruction to a dataset. However this can turn out to be a contrived result; when we evaluate the model on new data it can perform exceptionally poorly. The neural network effectively `memorises' the training data.

In order to combat this, regularisation methods such as dropout can be used. More data is always very useful at minimising the effects of overfitting, as well as the stacking of multiple layers. Overfitting is a phenomenon in machine learning that requires careful consideration \cite{DBLP:journals/corr/LiptonKEW15}.

\subsection{Rejected Methods of Anomaly Detection}
The support vector machine anomaly detection method was a considered method which was ultimately rejected. It operates by selecting a hyperplane through the data and classifying all data to one side anomalous. It is usually used with multi-variant data rather than simply training data plus some testing data. If used on the data collected it would be assuming our training data was a random walk, which is not helpful considering the cyclic nature of an motor. SVMs are most useful for binary classification, ie. determining whether a tumour is malignant of benign. It is also very good for detecting seasonality in a data set, but was not applicable to the task at hand given the lack of multi-variance in our data.

An ARIMA model of anomaly detection was also considered. The ARIMA model is 